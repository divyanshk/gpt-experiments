# GRPO Training Configuration - Cluster Mode
# Optimized for multi-GPU cluster training

mode: cluster
output_dir: ./grpo_results_cluster

# Training Hyperparameters
training:
  num_train_epochs: 3
  per_device_train_batch_size: 8
  gradient_accumulation_steps: 2
  learning_rate: 3.0e-5
  warmup_steps: 500

# Logging & Checkpointing
logging:
  logging_steps: 50
  save_steps: 1000
  log_level: passive
  disable_tqdm: false

# Data Loading
data:
  dataloader_num_workers: 4
  remove_unused_columns: false

# Optimization
optimization:
  fp16: true
  gradient_checkpointing: true

# GRPO Specific Parameters
grpo:
  generation_batch_size: 16  # Must be divisible by num_generations
  num_generations: 8         # Number of generations per prompt
  beta: 0.01                 # KL penalty coefficient (enables KL tracking)

# LoRA Configuration (optional)
lora:
  enabled: false
  r: 16
  lora_alpha: 32
  lora_dropout: 0.05
  target_modules: ["c_attn", "c_proj"]  # GPT-2 specific
