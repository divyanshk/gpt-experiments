# GRPO Training Configuration - Local Mode
# Optimized for development and testing on local hardware

mode: local
output_dir: ./grpo_results_local

# Training Hyperparameters
training:
  num_train_epochs: 1
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 1
  learning_rate: 5.0e-5
  warmup_steps: 0

# Logging & Checkpointing
logging:
  logging_steps: 10
  save_steps: 500
  log_level: passive
  disable_tqdm: false

# Data Loading
data:
  dataloader_num_workers: 0
  remove_unused_columns: false

# Optimization
optimization:
  optim: adamw_torch_fused    # Optimizer: adamw_torch_fused (faster on modern GPUs), adafactor, etc.
  fp16: false                 # Will be auto-set based on device
  gradient_checkpointing: true

# GRPO Specific Parameters
grpo:
  generation_batch_size: 8  # Must be divisible by num_generations
  num_generations: 8        # Number of generations per prompt
  beta: 0.01                # KL penalty coefficient (enables KL tracking)
  use_vllm: false           # Use vLLM for faster generation (requires vllm package)

  # Token-level vs Sequence-level Configuration
  loss_type: "dapo"         # Options: "grpo" (length-biased), "dr_grpo" (global constant),
                            #          "dapo" (default, active tokens), "bnpo" (local batch)
  importance_sampling_level: "sequence"  # Options: "token" (per-token ratios), "sequence" (averaged)

  # Advanced Token-level Settings
  top_entropy_quantile: null  # Keep top-œÅ quantile of tokens by entropy (null = all tokens)
  mask_truncated_completions: false  # Exclude truncated completions from loss

# LoRA Configuration (optional)
# Note: lora_alpha defaults to 2x the rank if not specified
lora:
  enabled: false
  r: 16                # LoRA rank (smaller = fewer params, less capacity)
  lora_alpha: 32       # Scaling factor (typically 2x rank)
  lora_dropout: 0.05
  target_modules: ["c_attn", "c_proj"]  # GPT-2: auto-detected, can override here
